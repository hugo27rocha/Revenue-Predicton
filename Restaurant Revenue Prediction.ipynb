{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining / Prospeção de Dados\n",
    "\n",
    "\n",
    "# Project - Classification/Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Tools\n",
    "\n",
    "In this project you should use [Python 3](https://www.python.org), [Jupyter Notebook](http://jupyter.org) and **[Scikit-learn](http://scikit-learn.org/stable/). You are also allowed to use [Orange3](https://orange.biolab.si).**\n",
    "\n",
    "The dataset to be analysed is **`RestaurantsRevenue.csv`**, a modified version of the test dataset used in Kaggle's competition [\"Restaurant Revenue Prediction\"](https://www.kaggle.com/c/restaurant-revenue-prediction/overview). \n",
    "\n",
    "**This project challenges you twice** by asking you to tackle a\n",
    "1. **Regression Task**: predict the revenue, and a\n",
    "2. **Classification Task**: predict a revenue category.\n",
    "\n",
    "The available variables are:\n",
    "\n",
    "* **Id :** Restaurant id. \n",
    "* **Open Date :** opening date for a restaurant\n",
    "* **City :** City that the restaurant is in. Note that there are unicode in the names. \n",
    "* **City Group:** Type of the city. Big cities, or Other. \n",
    "* **Type:** Type of the restaurant. FC: Food Court, IL: Inline, DT: Drive Thru, MB: Mobile\n",
    "* **P1, P2 - P37:** There are three categories of these obfuscated data. Demographic data are gathered from third party providers with GIS systems. These include population in any given area, age and gender distribution, development scales. Real estate data mainly relate to the m2 of the location, front facade of the location, car park availability. Commercial data mainly include the existence of points of interest including schools, banks, other QSR operators.\n",
    "\n",
    "\n",
    "The targets are:\n",
    "1. **`Revenue`:** The revenue column indicates a (transformed) revenue of the restaurant in a given year and is the target of predictive analysis. Please note that the values are transformed so they don't mean real dollar values. \n",
    "2. **`RevenueCategory`** - the revenue category, where price can be below 12000 (\"<12K\"), between 12000 and 20000 (\"12K-20K\"), or above 20000 (\">20K\"). This is the target variable that you're trying to predict in the classification task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Identification\n",
    "\n",
    "**GROUP 010**\n",
    "\n",
    "Students:\n",
    "\n",
    "* Student 1 - Hugo Rocha Nº48331\n",
    "* Student 2 - Leonardo Ricardo Nº55047\n",
    "* Student 3 - Filipa Santos Nº57167"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import decomposition\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import VotingClassifier, VotingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > - Before going through the 3 tasks above, we will explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('RestaurantsReveneu.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Open Date</th>\n",
       "      <th>City</th>\n",
       "      <th>City Group</th>\n",
       "      <th>Type</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>...</th>\n",
       "      <th>P29</th>\n",
       "      <th>P30</th>\n",
       "      <th>P31</th>\n",
       "      <th>P32</th>\n",
       "      <th>P33</th>\n",
       "      <th>P34</th>\n",
       "      <th>P35</th>\n",
       "      <th>P36</th>\n",
       "      <th>P37</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>01/22/2011</td>\n",
       "      <td>Niğde</td>\n",
       "      <td>Other</td>\n",
       "      <td>FC</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10033.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>03/18/2011</td>\n",
       "      <td>Konya</td>\n",
       "      <td>Other</td>\n",
       "      <td>IL</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9355.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10/30/2013</td>\n",
       "      <td>Ankara</td>\n",
       "      <td>Big Cities</td>\n",
       "      <td>FC</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11353.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>05/06/2013</td>\n",
       "      <td>Kocaeli</td>\n",
       "      <td>Other</td>\n",
       "      <td>IL</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10828.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>07/31/2013</td>\n",
       "      <td>Afyonkarahisar</td>\n",
       "      <td>Other</td>\n",
       "      <td>FC</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9354.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   Open Date            City  City Group Type  P1   P2   P3   P4  P5  \\\n",
       "0   0  01/22/2011           Niğde       Other   FC   1  4.0  4.0  4.0   1   \n",
       "1   1  03/18/2011           Konya       Other   IL   3  4.0  4.0  4.0   2   \n",
       "2   2  10/30/2013          Ankara  Big Cities   FC   3  4.0  4.0  4.0   2   \n",
       "3   3  05/06/2013         Kocaeli       Other   IL   2  4.0  4.0  4.0   2   \n",
       "4   4  07/31/2013  Afyonkarahisar       Other   FC   2  4.0  4.0  4.0   1   \n",
       "\n",
       "   ...  P29  P30  P31  P32  P33  P34  P35  P36  P37  revenue  \n",
       "0  ...  3.0    0    0    0    0    0    0    0    0  10033.0  \n",
       "1  ...  3.0    0    0    0    0    0    0    0    0   9355.0  \n",
       "2  ...  3.0    0    0    0    0    0    0    0    0  11353.0  \n",
       "3  ...  3.0    0    4    0    0    0    0    0    0  10828.0  \n",
       "4  ...  3.0    0    0    0    0    0    0    0    0   9354.0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 43)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 43 columns):\n",
      "Id            100000 non-null int64\n",
      "Open Date     100000 non-null object\n",
      "City          100000 non-null object\n",
      "City Group    100000 non-null object\n",
      "Type          100000 non-null object\n",
      "P1            100000 non-null int64\n",
      "P2            100000 non-null float64\n",
      "P3            100000 non-null float64\n",
      "P4            100000 non-null float64\n",
      "P5            100000 non-null int64\n",
      "P6            100000 non-null int64\n",
      "P7            100000 non-null int64\n",
      "P8            100000 non-null int64\n",
      "P9            100000 non-null int64\n",
      "P10           100000 non-null int64\n",
      "P11           100000 non-null int64\n",
      "P12           100000 non-null int64\n",
      "P13           100000 non-null float64\n",
      "P14           100000 non-null int64\n",
      "P15           100000 non-null int64\n",
      "P16           100000 non-null int64\n",
      "P17           100000 non-null int64\n",
      "P18           100000 non-null int64\n",
      "P19           100000 non-null int64\n",
      "P20           100000 non-null int64\n",
      "P21           100000 non-null int64\n",
      "P22           100000 non-null int64\n",
      "P23           100000 non-null int64\n",
      "P24           100000 non-null int64\n",
      "P25           100000 non-null int64\n",
      "P26           100000 non-null float64\n",
      "P27           100000 non-null float64\n",
      "P28           100000 non-null float64\n",
      "P29           100000 non-null float64\n",
      "P30           100000 non-null int64\n",
      "P31           100000 non-null int64\n",
      "P32           100000 non-null int64\n",
      "P33           100000 non-null int64\n",
      "P34           100000 non-null int64\n",
      "P35           100000 non-null int64\n",
      "P36           100000 non-null int64\n",
      "P37           100000 non-null int64\n",
      "revenue       100000 non-null float64\n",
      "dtypes: float64(9), int64(30), object(4)\n",
      "memory usage: 32.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>P7</th>\n",
       "      <th>P8</th>\n",
       "      <th>P9</th>\n",
       "      <th>...</th>\n",
       "      <th>P29</th>\n",
       "      <th>P30</th>\n",
       "      <th>P31</th>\n",
       "      <th>P32</th>\n",
       "      <th>P33</th>\n",
       "      <th>P34</th>\n",
       "      <th>P35</th>\n",
       "      <th>P36</th>\n",
       "      <th>P37</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>49999.500000</td>\n",
       "      <td>4.088030</td>\n",
       "      <td>4.428085</td>\n",
       "      <td>4.215325</td>\n",
       "      <td>4.396025</td>\n",
       "      <td>1.989590</td>\n",
       "      <td>2.881900</td>\n",
       "      <td>5.30051</td>\n",
       "      <td>4.93100</td>\n",
       "      <td>5.251380</td>\n",
       "      <td>...</td>\n",
       "      <td>3.084000</td>\n",
       "      <td>2.083300</td>\n",
       "      <td>1.193330</td>\n",
       "      <td>1.942640</td>\n",
       "      <td>0.987430</td>\n",
       "      <td>2.108670</td>\n",
       "      <td>1.832830</td>\n",
       "      <td>1.968890</td>\n",
       "      <td>0.973500</td>\n",
       "      <td>14698.061620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28867.657797</td>\n",
       "      <td>2.812963</td>\n",
       "      <td>1.428865</td>\n",
       "      <td>0.842161</td>\n",
       "      <td>1.035827</td>\n",
       "      <td>1.065314</td>\n",
       "      <td>1.531429</td>\n",
       "      <td>2.17858</td>\n",
       "      <td>1.71849</td>\n",
       "      <td>1.702632</td>\n",
       "      <td>...</td>\n",
       "      <td>1.783927</td>\n",
       "      <td>4.309479</td>\n",
       "      <td>2.307944</td>\n",
       "      <td>3.971298</td>\n",
       "      <td>1.534808</td>\n",
       "      <td>4.685414</td>\n",
       "      <td>3.228769</td>\n",
       "      <td>3.805773</td>\n",
       "      <td>1.677267</td>\n",
       "      <td>6705.081965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6271.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24999.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10143.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>49999.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12951.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>74999.250000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>16923.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99999.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>52294.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Id             P1             P2             P3  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean    49999.500000       4.088030       4.428085       4.215325   \n",
       "std     28867.657797       2.812963       1.428865       0.842161   \n",
       "min         0.000000       1.000000       1.000000       0.000000   \n",
       "25%     24999.750000       2.000000       3.750000       4.000000   \n",
       "50%     49999.500000       3.000000       5.000000       4.000000   \n",
       "75%     74999.250000       4.000000       5.000000       4.000000   \n",
       "max     99999.000000      15.000000       7.500000       6.000000   \n",
       "\n",
       "                  P4             P5             P6            P7  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.00000   \n",
       "mean        4.396025       1.989590       2.881900       5.30051   \n",
       "std         1.035827       1.065314       1.531429       2.17858   \n",
       "min         2.000000       1.000000       1.000000       1.00000   \n",
       "25%         4.000000       1.000000       2.000000       5.00000   \n",
       "50%         4.000000       2.000000       2.000000       5.00000   \n",
       "75%         5.000000       2.000000       4.000000       5.00000   \n",
       "max         7.500000       6.000000      10.000000      10.00000   \n",
       "\n",
       "                 P8             P9  ...            P29            P30  \\\n",
       "count  100000.00000  100000.000000  ...  100000.000000  100000.000000   \n",
       "mean        4.93100       5.251380  ...       3.084000       2.083300   \n",
       "std         1.71849       1.702632  ...       1.783927       4.309479   \n",
       "min         1.00000       4.000000  ...       0.000000       0.000000   \n",
       "25%         4.00000       4.000000  ...       2.000000       0.000000   \n",
       "50%         5.00000       5.000000  ...       3.000000       0.000000   \n",
       "75%         5.00000       5.000000  ...       3.000000       3.000000   \n",
       "max        10.00000      10.000000  ...      10.000000      25.000000   \n",
       "\n",
       "                 P31            P32            P33            P34  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        1.193330       1.942640       0.987430       2.108670   \n",
       "std         2.307944       3.971298       1.534808       4.685414   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         1.000000       3.000000       2.000000       3.000000   \n",
       "max        15.000000      25.000000       6.000000      30.000000   \n",
       "\n",
       "                 P35            P36            P37        revenue  \n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000  \n",
       "mean        1.832830       1.968890       0.973500   14698.061620  \n",
       "std         3.228769       3.805773       1.677267    6705.081965  \n",
       "min         0.000000       0.000000       0.000000    6271.000000  \n",
       "25%         0.000000       0.000000       0.000000   10143.000000  \n",
       "50%         0.000000       0.000000       0.000000   12951.000000  \n",
       "75%         4.000000       3.000000       2.000000   16923.000000  \n",
       "max        15.000000      20.000000       8.000000   52294.000000  \n",
       "\n",
       "[8 rows x 39 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe() #checking descriptive statistics of the numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Open Date'] = pd.to_datetime(df['Open Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Open Date'].nunique() #checking unique dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Open Date']=df['Open Date'].dt.strftime('%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        01-2011\n",
       "1        03-2011\n",
       "2        10-2013\n",
       "3        05-2013\n",
       "4        07-2013\n",
       "          ...   \n",
       "99995    01-2000\n",
       "99996    07-2011\n",
       "99997    12-2012\n",
       "99998    10-2013\n",
       "99999    10-2010\n",
       "Name: Open Date, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting to just %m-%Y format since we don't consider the day to be relevant\n",
    "df['Open Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_date = df[\"Open Date\"].min()\n",
    "last_date = df[\"Open Date\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01-1999'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12-2013'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "İstanbul          34087\n",
       "Ankara             8720\n",
       "İzmir              6465\n",
       "Antalya            5911\n",
       "Kocaeli            4364\n",
       "Mersin             2735\n",
       "Adana              2514\n",
       "Balıkesir          2463\n",
       "Bursa              2441\n",
       "Muğla              1823\n",
       "Aydın              1617\n",
       "Tekirdağ           1577\n",
       "Konya              1576\n",
       "Gaziantep          1487\n",
       "Edirne             1230\n",
       "Manisa             1227\n",
       "Çanakkale           965\n",
       "Denizli             964\n",
       "Diyarbakır          954\n",
       "Hatay               951\n",
       "Zonguldak           926\n",
       "Eskişehir           900\n",
       "Trabzon             660\n",
       "Aksaray             650\n",
       "Bolu                631\n",
       "Yalova              630\n",
       "Kırıkkale           622\n",
       "Malatya             616\n",
       "Mardin              610\n",
       "Şanlıurfa           609\n",
       "Sakarya             604\n",
       "Batman              604\n",
       "Rize                345\n",
       "Artvin              344\n",
       "Bilecik             339\n",
       "Afyonkarahisar      331\n",
       "Nevşehir            328\n",
       "Sivas               326\n",
       "Samsun              324\n",
       "Kayseri             323\n",
       "Kırşehir            319\n",
       "Erzincan            319\n",
       "Erzurum             317\n",
       "Ordu                317\n",
       "Siirt               315\n",
       "Kahramanmaraş       315\n",
       "Giresun             310\n",
       "Niğde               310\n",
       "Çankırı             309\n",
       "Çorum               304\n",
       "Isparta             304\n",
       "Kütahya             304\n",
       "Düzce               303\n",
       "Tanımsız            298\n",
       "Uşak                293\n",
       "Kars                289\n",
       "Kırklareli          281\n",
       "Name: City, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.City.value_counts() #checking the different cities in turkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other         50728\n",
       "Big Cities    49272\n",
       "Name: City Group, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['City Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FC    57019\n",
       "IL    40447\n",
       "DT     2244\n",
       "MB      290\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > - **1.1 Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id            0\n",
       "Open Date     0\n",
       "City          0\n",
       "City Group    0\n",
       "Type          0\n",
       "P1            0\n",
       "P2            0\n",
       "P3            0\n",
       "P4            0\n",
       "P5            0\n",
       "P6            0\n",
       "P7            0\n",
       "P8            0\n",
       "P9            0\n",
       "P10           0\n",
       "P11           0\n",
       "P12           0\n",
       "P13           0\n",
       "P14           0\n",
       "P15           0\n",
       "P16           0\n",
       "P17           0\n",
       "P18           0\n",
       "P19           0\n",
       "P20           0\n",
       "P21           0\n",
       "P22           0\n",
       "P23           0\n",
       "P24           0\n",
       "P25           0\n",
       "P26           0\n",
       "P27           0\n",
       "P28           0\n",
       "P29           0\n",
       "P30           0\n",
       "P31           0\n",
       "P32           0\n",
       "P33           0\n",
       "P34           0\n",
       "P35           0\n",
       "P36           0\n",
       "P37           0\n",
       "revenue       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 - unique values: 9\n",
      "P2 - unique values: 9\n",
      "P3 - unique values: 7\n",
      "P4 - unique values: 7\n",
      "P5 - unique values: 6\n",
      "P6 - unique values: 8\n",
      "P7 - unique values: 7\n",
      "P8 - unique values: 8\n",
      "P9 - unique values: 5\n",
      "P10 - unique values: 4\n",
      "P11 - unique values: 8\n",
      "P12 - unique values: 7\n",
      "P13 - unique values: 5\n",
      "P14 - unique values: 10\n",
      "P15 - unique values: 9\n",
      "P16 - unique values: 9\n",
      "P17 - unique values: 10\n",
      "P18 - unique values: 9\n",
      "P19 - unique values: 9\n",
      "P20 - unique values: 9\n",
      "P21 - unique values: 9\n",
      "P22 - unique values: 5\n",
      "P23 - unique values: 9\n",
      "P24 - unique values: 9\n",
      "P25 - unique values: 9\n",
      "P26 - unique values: 10\n",
      "P27 - unique values: 10\n",
      "P28 - unique values: 9\n",
      "P29 - unique values: 8\n",
      "P30 - unique values: 10\n",
      "P31 - unique values: 10\n",
      "P32 - unique values: 10\n",
      "P33 - unique values: 7\n",
      "P34 - unique values: 11\n",
      "P35 - unique values: 7\n",
      "P36 - unique values: 10\n",
      "P37 - unique values: 8\n"
     ]
    }
   ],
   "source": [
    "#we will take a closer look at columns starting with \"P\" which represent obfuscated data\n",
    "for col in df.columns:\n",
    "    if col[0] == 'P':\n",
    "        print (col, '- unique values:', len(df[col].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "obfuscated_cols = []\n",
    "for col in df.columns:\n",
    "    if col[0] == 'P':\n",
    "        obfuscated_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92414"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(~df[obfuscated_cols].all(1)).sum() \n",
    "#out of 100000 rows, 92414 have at least one zero in the obfuscated features P1-037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ob = df[obfuscated_cols] #getting a dataframe with just the features starting with \"P\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_obfuscated_cols = df_ob[df_ob == 0].count(axis=0)/df_ob.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "P1      0.000\n",
       "P28     0.000\n",
       "P23     0.000\n",
       "P22     0.000\n",
       "P21     0.000\n",
       "P20     0.000\n",
       "P13     0.000\n",
       "P12     0.000\n",
       "P11     0.000\n",
       "P19     0.000\n",
       "P9      0.000\n",
       "P8      0.000\n",
       "P7      0.000\n",
       "P2      0.000\n",
       "P6      0.000\n",
       "P5      0.000\n",
       "P4      0.000\n",
       "P10     0.000\n",
       "P3      0.318\n",
       "P29     3.083\n",
       "P31    65.566\n",
       "P30    65.596\n",
       "P36    65.662\n",
       "P14    65.734\n",
       "P25    65.738\n",
       "P24    65.766\n",
       "P15    65.772\n",
       "P35    65.776\n",
       "P26    65.784\n",
       "P32    65.787\n",
       "P33    65.791\n",
       "P17    65.792\n",
       "P34    65.832\n",
       "P18    65.980\n",
       "P37    66.029\n",
       "P16    66.094\n",
       "P27    66.193\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_obfuscated_cols.sort_values() #getting the missing values (when an instance is zero) for each of the \"P\" features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['P14',\n",
       " 'P15',\n",
       " 'P16',\n",
       " 'P17',\n",
       " 'P18',\n",
       " 'P24',\n",
       " 'P25',\n",
       " 'P26',\n",
       " 'P27',\n",
       " 'P30',\n",
       " 'P31',\n",
       " 'P32',\n",
       " 'P33',\n",
       " 'P34',\n",
       " 'P35',\n",
       " 'P36',\n",
       " 'P37']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping features with more than 65% of missing values\n",
    "missing_obfuscated_cols_to_drop = missing_obfuscated_cols[missing_obfuscated_cols>65].index\n",
    "missing_obfuscated_cols_to_drop = list(missing_obfuscated_cols_to_drop)\n",
    "missing_obfuscated_cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(missing_obfuscated_cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing missing values of P3 and P29 with the mean\n",
    "df['P3']=df['P3'].replace(0,df['P3'].mean())\n",
    "df['P29']=df['P3'].replace(0,df['P29'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > - **P1, P2 - P37**: There are three categories of these obfuscated data:\n",
    "    - Demographic data (population in any given area, age and gender distribution, development scales)\n",
    "    - Real estate data (m2 of the location, front facade of the location, car park availability)\n",
    "    - Commercial data (points of interest including schools, banks, other QSR operators)\n",
    "\n",
    "  > - Although we know that there are 3 categories in the obfuscated data represented from columns P1-P37, we are not able to choose an imputation method for those with high missing values (>65%) at the risk of changing the original data. A *zero* could represent that, for example, there are no points of interest in a given city or simply they were not measured. We have chosen to drop from our analysis features whose missing values represent more than 65%, those are: P31, P30, P36, P14, P25, P24, P15, P35, P26, P32, P33, P17, P34, P18, P37, P16 and P27. As such, we will end up with only 2 of the obfuscated features with missing values, **P3 and P29**, with zeros representing 0.3% and 3.1%. Given these low percetages, we will **replace the zeros with the mean**. \n",
    "\n",
    "  > - One call out about these obfuscated features, whose instances seem to between 1 and 25 could be that, for example, they could be the order of the answers in a formulaire e.g. for question 1 (P1), Restaurant with ID 1 has answered the first option 1. We should again, keep in mind, we are just making this assumption. \n",
    "This would highlight the fact that this representation may cause some issues since some Machine Learning algorithms will assume that two nearby values (answering option 1 and option 2) are more similar than two distant (answering option 1 and option 25). It may not be the case for the obfuscated features. To fix this, we will create **binary attributes per obfuscated feature** e.g. one attribute equal to 1 when P1 is \"1\" (and 0 otherwise). We will additionally use **One Hot Encoding for the categorical features** we have analysed: City, City Group and Type as well as the time series data (Open Date). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > - **1.2 Feature selection**\n",
    " \n",
    "  > - **1.2.a) Categorical Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > - As mentioned in *1.1* we will create dummy varibles for the features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df.drop(['Id','revenue'],axis=1) #dropping the unique identifier column and the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat_encoder = OneHotEncoder()\n",
    "df_cat_1hot = cat_encoder.fit_transform(df_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 350)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cat_1hot.shape #sanity check, we have now 350 new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0_01-1999', 'x0_01-2000', 'x0_01-2005', 'x0_01-2009',\n",
       "       'x0_01-2011', 'x0_01-2012', 'x0_01-2013', 'x0_01-2014',\n",
       "       'x0_02-1998', 'x0_02-2000', 'x0_02-2004', 'x0_02-2007',\n",
       "       'x0_02-2009', 'x0_02-2010', 'x0_02-2011', 'x0_02-2012',\n",
       "       'x0_02-2013', 'x0_03-1996', 'x0_03-1998', 'x0_03-2002',\n",
       "       'x0_03-2005', 'x0_03-2006', 'x0_03-2007', 'x0_03-2008',\n",
       "       'x0_03-2009', 'x0_03-2010', 'x0_03-2011', 'x0_03-2012',\n",
       "       'x0_03-2013', 'x0_04-1997', 'x0_04-1998', 'x0_04-2000',\n",
       "       'x0_04-2006', 'x0_04-2007', 'x0_04-2008', 'x0_04-2009',\n",
       "       'x0_04-2010', 'x0_04-2011', 'x0_04-2012', 'x0_04-2013',\n",
       "       'x0_05-1995', 'x0_05-1997', 'x0_05-1998', 'x0_05-2006',\n",
       "       'x0_05-2007', 'x0_05-2008', 'x0_05-2009', 'x0_05-2010',\n",
       "       'x0_05-2011', 'x0_05-2012', 'x0_05-2013', 'x0_06-1996',\n",
       "       'x0_06-1997', 'x0_06-1999', 'x0_06-2000', 'x0_06-2001',\n",
       "       'x0_06-2003', 'x0_06-2006', 'x0_06-2007', 'x0_06-2009',\n",
       "       'x0_06-2010', 'x0_06-2011', 'x0_06-2012', 'x0_06-2013',\n",
       "       'x0_07-1997', 'x0_07-2000', 'x0_07-2004', 'x0_07-2007',\n",
       "       'x0_07-2008', 'x0_07-2009', 'x0_07-2010', 'x0_07-2011',\n",
       "       'x0_07-2012', 'x0_07-2013', 'x0_08-1995', 'x0_08-1996',\n",
       "       'x0_08-1999', 'x0_08-2000', 'x0_08-2004', 'x0_08-2005',\n",
       "       'x0_08-2006', 'x0_08-2007', 'x0_08-2008', 'x0_08-2009',\n",
       "       'x0_08-2010', 'x0_08-2011', 'x0_08-2012', 'x0_08-2013',\n",
       "       'x0_09-1996', 'x0_09-1997', 'x0_09-1998', 'x0_09-1999',\n",
       "       'x0_09-2001', 'x0_09-2003', 'x0_09-2005', 'x0_09-2006',\n",
       "       'x0_09-2007', 'x0_09-2008', 'x0_09-2009', 'x0_09-2010',\n",
       "       'x0_09-2011', 'x0_09-2012', 'x0_09-2013', 'x0_10-1995',\n",
       "       'x0_10-1999', 'x0_10-2000', 'x0_10-2004', 'x0_10-2006',\n",
       "       'x0_10-2007', 'x0_10-2008', 'x0_10-2009', 'x0_10-2010',\n",
       "       'x0_10-2011', 'x0_10-2012', 'x0_10-2013', 'x0_11-1995',\n",
       "       'x0_11-1999', 'x0_11-2000', 'x0_11-2006', 'x0_11-2007',\n",
       "       'x0_11-2008', 'x0_11-2009', 'x0_11-2010', 'x0_11-2011',\n",
       "       'x0_11-2012', 'x0_11-2013', 'x0_12-1997', 'x0_12-1998',\n",
       "       'x0_12-1999', 'x0_12-2003', 'x0_12-2004', 'x0_12-2005',\n",
       "       'x0_12-2006', 'x0_12-2007', 'x0_12-2008', 'x0_12-2009',\n",
       "       'x0_12-2010', 'x0_12-2011', 'x0_12-2012', 'x0_12-2013', 'x1_Adana',\n",
       "       'x1_Afyonkarahisar', 'x1_Aksaray', 'x1_Ankara', 'x1_Antalya',\n",
       "       'x1_Artvin', 'x1_Aydın', 'x1_Balıkesir', 'x1_Batman', 'x1_Bilecik',\n",
       "       'x1_Bolu', 'x1_Bursa', 'x1_Denizli', 'x1_Diyarbakır', 'x1_Düzce',\n",
       "       'x1_Edirne', 'x1_Erzincan', 'x1_Erzurum', 'x1_Eskişehir',\n",
       "       'x1_Gaziantep', 'x1_Giresun', 'x1_Hatay', 'x1_Isparta',\n",
       "       'x1_Kahramanmaraş', 'x1_Kars', 'x1_Kayseri', 'x1_Kocaeli',\n",
       "       'x1_Konya', 'x1_Kütahya', 'x1_Kırklareli', 'x1_Kırıkkale',\n",
       "       'x1_Kırşehir', 'x1_Malatya', 'x1_Manisa', 'x1_Mardin', 'x1_Mersin',\n",
       "       'x1_Muğla', 'x1_Nevşehir', 'x1_Niğde', 'x1_Ordu', 'x1_Rize',\n",
       "       'x1_Sakarya', 'x1_Samsun', 'x1_Siirt', 'x1_Sivas', 'x1_Tanımsız',\n",
       "       'x1_Tekirdağ', 'x1_Trabzon', 'x1_Uşak', 'x1_Yalova',\n",
       "       'x1_Zonguldak', 'x1_Çanakkale', 'x1_Çankırı', 'x1_Çorum',\n",
       "       'x1_İstanbul', 'x1_İzmir', 'x1_Şanlıurfa', 'x2_Big Cities',\n",
       "       'x2_Other', 'x3_DT', 'x3_FC', 'x3_IL', 'x3_MB', 'x4_1', 'x4_2',\n",
       "       'x4_3', 'x4_4', 'x4_5', 'x4_6', 'x4_9', 'x4_12', 'x4_15', 'x5_1.0',\n",
       "       'x5_1.5', 'x5_2.0', 'x5_3.0', 'x5_4.0', 'x5_4.5', 'x5_5.0',\n",
       "       'x5_6.0', 'x5_7.5', 'x6_2.0', 'x6_3.0', 'x6_4.0', 'x6_4.215325',\n",
       "       'x6_4.5', 'x6_5.0', 'x6_6.0', 'x7_2.0', 'x7_3.0', 'x7_4.0',\n",
       "       'x7_4.5', 'x7_5.0', 'x7_6.0', 'x7_7.5', 'x8_1', 'x8_2', 'x8_3',\n",
       "       'x8_4', 'x8_5', 'x8_6', 'x9_1', 'x9_2', 'x9_3', 'x9_4', 'x9_5',\n",
       "       'x9_6', 'x9_8', 'x9_10', 'x10_1', 'x10_2', 'x10_3', 'x10_4',\n",
       "       'x10_5', 'x10_6', 'x10_10', 'x11_1', 'x11_2', 'x11_3', 'x11_4',\n",
       "       'x11_5', 'x11_6', 'x11_8', 'x11_10', 'x12_4', 'x12_5', 'x12_6',\n",
       "       'x12_8', 'x12_10', 'x13_4', 'x13_5', 'x13_8', 'x13_10', 'x14_1',\n",
       "       'x14_2', 'x14_3', 'x14_4', 'x14_5', 'x14_6', 'x14_8', 'x14_10',\n",
       "       'x15_2', 'x15_3', 'x15_4', 'x15_5', 'x15_6', 'x15_8', 'x15_10',\n",
       "       'x16_3.0', 'x16_4.0', 'x16_5.0', 'x16_6.0', 'x16_7.5', 'x17_1',\n",
       "       'x17_2', 'x17_3', 'x17_4', 'x17_5', 'x17_10', 'x17_15', 'x17_20',\n",
       "       'x17_25', 'x18_1', 'x18_2', 'x18_3', 'x18_4', 'x18_5', 'x18_6',\n",
       "       'x18_9', 'x18_12', 'x18_15', 'x19_1', 'x19_2', 'x19_3', 'x19_4',\n",
       "       'x19_5', 'x19_6', 'x19_9', 'x19_12', 'x19_15', 'x20_1', 'x20_2',\n",
       "       'x20_3', 'x20_4', 'x20_5', 'x21_1', 'x21_2', 'x21_3', 'x21_4',\n",
       "       'x21_5', 'x21_10', 'x21_15', 'x21_20', 'x21_25', 'x22_1.0',\n",
       "       'x22_2.0', 'x22_2.5', 'x22_3.0', 'x22_4.0', 'x22_5.0', 'x22_7.5',\n",
       "       'x22_10.0', 'x22_12.5', 'x23_2.0', 'x23_3.0', 'x23_4.0',\n",
       "       'x23_4.215325', 'x23_4.5', 'x23_5.0', 'x23_6.0'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_encoder.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(df_cat_1hot.toarray(), columns=cat_encoder.get_feature_names())\n",
    "#getting the dataframe with the one hot encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0_01-1999</th>\n",
       "      <th>x0_01-2000</th>\n",
       "      <th>x0_01-2005</th>\n",
       "      <th>x0_01-2009</th>\n",
       "      <th>x0_01-2011</th>\n",
       "      <th>x0_01-2012</th>\n",
       "      <th>x0_01-2013</th>\n",
       "      <th>x0_01-2014</th>\n",
       "      <th>x0_02-1998</th>\n",
       "      <th>x0_02-2000</th>\n",
       "      <th>...</th>\n",
       "      <th>x22_7.5</th>\n",
       "      <th>x22_10.0</th>\n",
       "      <th>x22_12.5</th>\n",
       "      <th>x23_2.0</th>\n",
       "      <th>x23_3.0</th>\n",
       "      <th>x23_4.0</th>\n",
       "      <th>x23_4.215325</th>\n",
       "      <th>x23_4.5</th>\n",
       "      <th>x23_5.0</th>\n",
       "      <th>x23_6.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 350 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   x0_01-1999  x0_01-2000  x0_01-2005  x0_01-2009  x0_01-2011  x0_01-2012  \\\n",
       "0         0.0         0.0         0.0         0.0         1.0         0.0   \n",
       "1         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   x0_01-2013  x0_01-2014  x0_02-1998  x0_02-2000  ...  x22_7.5  x22_10.0  \\\n",
       "0         0.0         0.0         0.0         0.0  ...      0.0       0.0   \n",
       "1         0.0         0.0         0.0         0.0  ...      0.0       0.0   \n",
       "\n",
       "   x22_12.5  x23_2.0  x23_3.0  x23_4.0  x23_4.215325  x23_4.5  x23_5.0  \\\n",
       "0       0.0      0.0      0.0      1.0           0.0      0.0      0.0   \n",
       "1       0.0      0.0      0.0      1.0           0.0      0.0      0.0   \n",
       "\n",
       "   x23_6.0  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "\n",
       "[2 rows x 350 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > - **1.3 Class Imbalance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.revenue.hist(); #visually checking the distribution of 'revenue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52294.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.revenue.max() #to get the last value for the bin numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the new feature for the classification task, with 3 different classes\n",
    "df['revenue_category'] = pd.cut(df.revenue,\n",
    "                               bins=[0, 12000, 20000, 52295], \n",
    "                                right=False,\n",
    "                                labels = ['<12K','12K-20K','>20K'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue</th>\n",
       "      <th>revenue_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12K-20K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12K-20K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      revenue revenue_category\n",
       "1389  12000.0          12K-20K\n",
       "1471  12000.0          12K-20K"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.revenue==12000][['revenue','revenue_category']].head(2) #sanity check that revenue=12000 belongs to class 12K-20K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue</th>\n",
       "      <th>revenue_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74427</th>\n",
       "      <td>52294.0</td>\n",
       "      <td>&gt;20K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       revenue revenue_category\n",
       "74427  52294.0             >20K"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.revenue==df.revenue.max()][['revenue','revenue_category']] #sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12K-20K</th>\n",
       "      <td>43.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;12K</th>\n",
       "      <td>43.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&gt;20K</th>\n",
       "      <td>12.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         revenue_category\n",
       "12K-20K             43.94\n",
       "<12K                43.40\n",
       ">20K                12.66"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking class proportions across the whole dataset\n",
    "pd.DataFrame(np.round(df['revenue_category'].value_counts(normalize=True) * 100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Open Date</th>\n",
       "      <th>City</th>\n",
       "      <th>City Group</th>\n",
       "      <th>Type</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>...</th>\n",
       "      <th>P13</th>\n",
       "      <th>P19</th>\n",
       "      <th>P20</th>\n",
       "      <th>P21</th>\n",
       "      <th>P22</th>\n",
       "      <th>P23</th>\n",
       "      <th>P28</th>\n",
       "      <th>P29</th>\n",
       "      <th>revenue</th>\n",
       "      <th>revenue_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Id, Open Date, City, City Group, Type, P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P19, P20, P21, P22, P23, P28, P29, revenue, revenue_category]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 27 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['revenue_category'].isnull().values] #checking for any null values on the df again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df['revenue_category'] #definiting the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > - For the Classification task we will select the target feature **'revenue_category'.**\n",
    "  We have applied the **stratified sampling method** to ensure that the right number of instances are sampled from each of the 3 categories (<12K, 12K-20K, >20K) such that the test set is **representative of the overall population** as we can confirm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train_cat, y_test_cat = train_test_split(X, df['revenue_category'], test_size = 0.2, stratify=df['revenue_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape is :  (80000, 350)\n",
      "X_test shape  is :  (20000, 350)\n",
      "y_train shape is :  (80000,)\n",
      "y_test shape is :  (20000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape is : \", X_train.shape)\n",
    "print(\"X_test shape  is : \", X_test.shape)\n",
    "print(\"y_train shape is : \", y_train_cat.shape)\n",
    "print(\"y_test shape is : \", y_test_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12K-20K</th>\n",
       "      <td>43.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;12K</th>\n",
       "      <td>43.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&gt;20K</th>\n",
       "      <td>12.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         revenue_category\n",
       "12K-20K             43.94\n",
       "<12K                43.40\n",
       ">20K                12.66"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.round(df['revenue_category'].value_counts(normalize=True) * 100,2)) #overall population proportion of the classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12K-20K</th>\n",
       "      <td>43.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;12K</th>\n",
       "      <td>43.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&gt;20K</th>\n",
       "      <td>12.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         revenue_category\n",
       "12K-20K             43.94\n",
       "<12K                43.40\n",
       ">20K                12.66"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.round(y_test_cat.value_counts(normalize=True) * 100,2)) #after splitting into train and test\n",
    "#The test set generated using stratified sampling has revenue category proportions similar to those in the full dataset we've previously seen above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > - Due to the high number of features present, we will apply the **PCA** to both the classification and regression datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "\n",
    "pca.fit(X_train)\n",
    "\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "dimension_chosen = np.argmax(cumsum>=0.80) + 1\n",
    "dimension_chosen #we're reducing the dimensions to just 57 while keeping 80% of the explained variance of the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = dimension_chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cat_pca = pca.fit_transform(X_train)\n",
    "X_test_cat_pca = pca.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_cat_pca shape is :  (80000, 57)\n",
      "X_test_cat_pca shape  is :  (20000, 57)\n",
      "y_train shape is :  (80000,)\n",
      "y_test shape is :  (20000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_cat_pca shape is : \", X_train_cat_pca.shape)\n",
    "print(\"X_test_cat_pca shape  is : \", X_test_cat_pca.shape)\n",
    "print(\"y_train shape is : \", y_train_cat.shape)\n",
    "print(\"y_test shape is : \", y_test_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > - For the Regression task we will select the target feature **'revenue'** and apply **PCA** similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rev, X_test_rev, y_train_rev, y_test_rev = train_test_split(X, df['revenue'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_rev shape is :  (80000, 350)\n",
      "X_test_rev shape  is :  (20000, 350)\n",
      "y_train_rev shape is :  (80000,)\n",
      "y_test_rev shape is :  (20000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_rev shape is : \", X_train_rev.shape)\n",
    "print(\"X_test_rev shape  is : \", X_test_rev.shape)\n",
    "print(\"y_train_rev shape is : \", y_train_rev.shape)\n",
    "print(\"y_test_rev shape is : \", y_test_rev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "\n",
    "pca.fit(X_train_rev)\n",
    "\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "dimension_chosen_rev = np.argmax(cumsum>=0.80) + 1\n",
    "dimension_chosen_rev "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = dimension_chosen_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rev_pca = pca.fit_transform(X_train_rev)\n",
    "X_test_rev_pca = pca.fit_transform(X_test_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_pca_rev shape is :  (80000, 56)\n",
      "X_test_pca_rev shape  is :  (20000, 56)\n",
      "y_train shape is :  (80000,)\n",
      "y_test shape is :  (20000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_pca_rev shape is : \", X_train_rev_pca.shape)\n",
    "print(\"X_test_pca_rev shape  is : \", X_test_rev_pca.shape)\n",
    "print(\"y_train shape is : \", y_train_cat.shape)\n",
    "print(\"y_test shape is : \", y_test_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning Simple Classifiers\n",
    "\n",
    "* Choose **`X` classifiers** (https://scikit-learn.org/stable/supervised_learning.html#supervised-learning).\n",
    "* Use **grid-search and stratified 10 fold cross-validation** to estimate the best parameters (https://scikit-learn.org/stable/model_selection.html#model-selection). \n",
    "* Present mean and standard deviation of accuracy, precision and recall.\n",
    "* Show confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ['accuracy','precision','recall'] #we will print out the mean and std for these 3 metrics for each parameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = StratifiedKFold(10) #using stratified 10 fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - For all the classification models, we will use **accuracy** as the scoring metric parameter in the GridSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **2.1. Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1, 'max_iter': 60, 'penalty': 'l1'}\n",
      "\n",
      "Confusion matrix:\n",
      "[[5967 2281  541]\n",
      " [2177 6503    0]\n",
      " [ 873   24 1634]]\n",
      "\n",
      "Accuracy 0.705 ( mean: 0.75 & std: 0.005 )\n",
      "Recall 0.705\n",
      "Precision 0.706\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters= dict(max_iter=[60, 80, 100, 120],  #choosing a parameter grid for the gridsearch\n",
    "                       C=[1,2,3], \n",
    "                       penalty=['l1','l2','elasticnet','none'])\n",
    "\n",
    "lr_clf = GridSearchCV(LogisticRegression(solver='saga'), #using gridsearch\n",
    "                   tuned_parameters, \n",
    "                   scoring='accuracy', \n",
    "                   cv=kfolds.split(X_train_cat_pca, y_train_cat))\n",
    "\n",
    "lr_clf.fit(X_train_cat_pca, y_train_cat)\n",
    "\n",
    "y_true, y_pred = y_test_cat, lr_clf.predict(X_test_cat_pca)\n",
    "\n",
    "print(\"Best parameters:\",lr_clf.cv_results_['params'][lr_clf.best_index_])\n",
    "print()\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print()\n",
    "\n",
    "print('Accuracy', np.round(accuracy_score(y_true,y_pred),3), #presenting the mean and standard deviation of accuracy\n",
    "      '( mean:', \n",
    "       np.round(lr_clf.cv_results_['mean_test_score'][lr_clf.best_index_],3), \n",
    "       '& std:',\n",
    "       np.round(lr_clf.cv_results_['std_test_score'][lr_clf.best_index_],3),\n",
    "      ')')\n",
    "\n",
    "print('Recall', np.round(recall_score(y_true,y_pred, average='weighted'),3)) #getting the recall score\n",
    "print('Precision', np.round(precision_score(y_true,y_pred, average='weighted'),3)) #getting the precision score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **2.2. Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'criterion': 'entropy', 'max_depth': 4, 'min_samples_split': 3}\n",
      "\n",
      "Confusion matrix:\n",
      "[[5748 2258  783]\n",
      " [2562 6104   14]\n",
      " [ 801   78 1652]]\n",
      "\n",
      "Accuracy 0.675 ( mean: 0.673 & std: 0.005 )\n",
      "Recall 0.675\n",
      "Precision 0.676\n"
     ]
    }
   ],
   "source": [
    "#using the same structure of the code in 2.1. for all the classification models\n",
    "tuned_parameters= dict(criterion=['entropy','gini'],\n",
    "                       max_depth= [4,6,8,10,15,20],\n",
    "                      min_samples_split = [2,3,4,5,6,7,8,9,10])\n",
    "\n",
    "dt_clf = GridSearchCV(\n",
    "        DecisionTreeClassifier(), \n",
    "        tuned_parameters, \n",
    "        scoring='accuracy',\n",
    "        cv=kfolds.split(X_train_cat_pca, y_train_cat))\n",
    "    \n",
    "dt_clf.fit(X_train_cat_pca, y_train_cat)\n",
    "\n",
    "y_true, y_pred = y_test_cat, dt_clf.predict(X_test_cat_pca)\n",
    "\n",
    "print(\"Best parameters:\",dt_clf.cv_results_['params'][dt_clf.best_index_])\n",
    "print()\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print()\n",
    "\n",
    "print('Accuracy', np.round(accuracy_score(y_true,y_pred),3), \n",
    "      '( mean:', \n",
    "       np.round(dt_clf.cv_results_['mean_test_score'][dt_clf.best_index_],3), \n",
    "       '& std:',\n",
    "       np.round(dt_clf.cv_results_['std_test_score'][dt_clf.best_index_],3),\n",
    "      ')')\n",
    "\n",
    "print('Recall', np.round(recall_score(y_true,y_pred, average='weighted'),3))\n",
    "print('Precision', np.round(precision_score(y_true,y_pred, average='weighted'),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - **1.3. SVC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 3, 'kernel': 'rbf'}\n",
      "\n",
      "Confusion matrix:\n",
      "[[6270 1964  555]\n",
      " [2229 6446    5]\n",
      " [ 889   23 1619]]\n",
      "\n",
      "Accuracy 0.717 mean: 0.812 & std: 0.005 )\n",
      "Recall 0.717\n",
      "Precision 0.719\n"
     ]
    }
   ],
   "source": [
    "#using the same structure of the code in 2.1. for all the classification models\n",
    "tuned_parameters= dict(kernel=['rbf','linear'],\n",
    "                       C= [1,2,3])\n",
    "\n",
    "svc_clf = GridSearchCV(\n",
    "        SVC(), \n",
    "        tuned_parameters, \n",
    "        scoring='accuracy',\n",
    "        cv=kfolds.split(X_train_cat_pca, y_train_cat))\n",
    "    \n",
    "svc_clf.fit(X_train_cat_pca, y_train_cat)\n",
    "\n",
    "y_true, y_pred = y_test_cat, svc_clf.predict(X_test_cat_pca)\n",
    "\n",
    "print(\"Best parameters:\",svc_clf.cv_results_['params'][svc_clf.best_index_])\n",
    "print()\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print()\n",
    "\n",
    "print('Accuracy', np.round(accuracy_score(y_true,y_pred),3), \n",
    "      'mean:', \n",
    "       np.round(svc_clf.cv_results_['mean_test_score'][svc_clf.best_index_],3), \n",
    "       '& std:',\n",
    "       np.round(svc_clf.cv_results_['std_test_score'][svc_clf.best_index_],3),\n",
    "      ')')\n",
    "\n",
    "print('Recall', np.round(recall_score(y_true,y_pred, average='weighted'),3))\n",
    "print('Precision', np.round(precision_score(y_true,y_pred, average='weighted'),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning Simple Regressors\n",
    "\n",
    "* Choose **`X` regressors** (https://scikit-learn.org/stable/supervised_learning.html#supervised-learning).\n",
    "* Use **grid-search and 10 fold cross-validation** to estimate the best parameters (https://scikit-learn.org/stable/model_selection.html#model-selection). \n",
    "* Use the mean absolute error regression loss, or other relevant metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - We have chosen to analyse **R squared** as our performance metric for the regression models in order to compare the simple regressors with the ensemble models in task 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">   - **3.1. Lasso Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 0.1, 'max_iter': 500, 'selection': 'random'}\n",
      "R squared: 0.791\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters= dict(alpha = [0,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], #defining parameter grid\n",
    "                       max_iter = [100,200,500,1000],\n",
    "                       selection = [\"cyclic\", \"random\"])\n",
    "\n",
    "lasso_rg = GridSearchCV(linear_model.Lasso(), #using grid search\n",
    "                       tuned_parameters,\n",
    "                       cv=10, #using 10 fold CV\n",
    "                       scoring='r2') #using r2 metric\n",
    "\n",
    "lasso_rg.fit(X_train_rev_pca, y_train_rev)\n",
    "\n",
    "print('Best parameters:', lasso_rg.best_params_) #getting best paramaters\n",
    "print('R squared:', np.round(lasso_rg.best_score_,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">   - **3.2. SVM Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 2, 'degree': 2, 'kernel': 'linear'}\n",
      "R squared: 0.478\n"
     ]
    }
   ],
   "source": [
    "#using the same structure of the code in 3.1. for all the regression models\n",
    "tuned_parameters= dict(kernel=['rbf','linear'],\n",
    "                       C= [1, 2,3],\n",
    "                      degree= [2, 3,4,5,6])\n",
    "\n",
    "\n",
    "svr_rg = GridSearchCV(SVR(), \n",
    "                       tuned_parameters,\n",
    "                       cv=10,\n",
    "                       scoring='r2')\n",
    "\n",
    "svr_rg.fit(X_train_rev_pca, y_train_rev)\n",
    "\n",
    "print('Best parameters:', svr_rg.best_params_)\n",
    "print('R squared:', np.round(svr_rg.best_score_,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'criterion': 'mse', 'max_depth': 4, 'min_samples_split': 2}\n",
      "R squared: 0.555\n"
     ]
    }
   ],
   "source": [
    "#using the same structure of the code in 3.1. for all the regression models\n",
    "tuned_parameters= dict(criterion=['mse','mae'],\n",
    "                       max_depth= np.arange(2, 15),\n",
    "                       min_samples_split = np.arange(2, 15))\n",
    "\n",
    "dt_rg = GridSearchCV(DecisionTreeRegressor(), \n",
    "                       tuned_parameters,\n",
    "                       cv=10,\n",
    "                       scoring='r2')\n",
    "\n",
    "dt_rg.fit(X_train_rev_pca, y_train_rev)\n",
    "\n",
    "print('Best parameters:', dt_rg.best_params_)\n",
    "print('R squared:', np.round(dt_rg.best_score_,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Voting Classifier/Regressor\n",
    "\n",
    "* Use a voting classifier (http://scikit-learn.org/stable/modules/ensemble.html#voting-classifier)/regressor(https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html) to combine the best results of the `X` classifiers/regressors from previous sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">   - **4.1.a) Voting Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.58\n",
      "Recall 0.58\n",
      "Precision 0.57\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression(lr_clf.cv_results_['params'][lr_clf.best_index_])\n",
    "clf2 = DecisionTreeClassifier(dt_clf.cv_results_['params'][dt_clf.best_index_])\n",
    "clf3 = SVC(svc_clf.cv_results_['params'][svc_clf.best_index_])\n",
    "\n",
    "voting_class_hard = VotingClassifier(estimators=[('logr', clf1), \n",
    "                                     ('rf', clf2), \n",
    "                                     ('svc', clf3)], \n",
    "                                     voting='hard')\n",
    "\n",
    "voting_class_hard = voting_class_hard.fit(X_train_cat_pca, y_train_cat)\n",
    "\n",
    "y_true, y_pred = y_test_cat, voting_class_hard.predict(X_test_cat_pca) \n",
    "\n",
    "print('Accuracy', np.round(accuracy_score(y_true,y_pred),3))\n",
    "print('Recall', np.round(recall_score(y_true,y_pred, average='weighted'),3))\n",
    "print('Precision', np.round(precision_score(y_true,y_pred, average='weighted'),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">   - **4.1.b) Voting Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.57\n"
     ]
    }
   ],
   "source": [
    "rg1 = linear_model.Lasso(lasso_rg.best_params_)\n",
    "rg2 = SVR(svr_rg.best_params_)\n",
    "rg3 = DecisionTreeRegressor(dt_rg.best_params_)\n",
    "\n",
    "\n",
    "voting_rg = VotingRegressor(estimators=[('lasso',rg1), \n",
    "                                     ('svr', rg2), \n",
    "                                     ('dtrg', rg3)])\n",
    "voting_rg = voting_rg.fit(X_train_rev_pca, y_train_rev)\n",
    "\n",
    "print('Best score:', np.round(voting_rg.score(X_test_rev_pca, y_test_rev),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. XGBoost \n",
    "\n",
    "* Use [XGBoost](https://www.kaggle.com/stuarthallows/using-xgboost-with-scikit-learn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  - **4.2.a) XGBoostClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.726\n",
      "Recall 0.726\n",
      "Precision 0.727\n"
     ]
    }
   ],
   "source": [
    "#using merror (mlticlass classification error rate) as the evaluation metric to fit xgboost\n",
    "xgb_clf = xgb.XGBClassifier(n_jobs=1, eval_metric='merror').fit(X_train_cat_pca, y_train_cat)\n",
    "\n",
    "y_true, y_pred = y_test_cat, xgb_clf.predict(X_test_cat_pca)\n",
    "\n",
    "print('Accuracy', np.round(accuracy_score(y_true,y_pred),3))\n",
    "print('Recall', np.round(recall_score(y_true,y_pred, average='weighted'),3))\n",
    "print('Precision', np.round(precision_score(y_true,y_pred, average='weighted'),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  - **4.2.b) XGBoostRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared:  0.682\n"
     ]
    }
   ],
   "source": [
    "xgb_rg = xgb.XGBRegressor(n_jobs=1, objectvie='reg:squarederror').fit(X_train_rev_pca, y_train)\n",
    "\n",
    "y_true, y_pred = y_test_rev, xgb_rg.predict(X_test_rev_pca)\n",
    "\n",
    "print('R squared: ', np.round(r2_score(y_true, y_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Random Forests\n",
    "\n",
    "* Use [Random Forests](http://scikit-learn.org/stable/modules/ensemble.html#random-forests)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  - **4.3.a) Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.709\n",
      "Recall 0.709\n",
      "Precision 0.713\n"
     ]
    }
   ],
   "source": [
    "tuned_parameters = {'n_estimators':[20,40,50,57],\n",
    "                    'max_depth':[5,10,15,20],\n",
    "                    'random_state':[42],\n",
    "                    'criterion':['gini','entropy']}\n",
    "\n",
    "clf = GridSearchCV(RandomForestClassifier(), \n",
    "                       tuned_parameters, \n",
    "                       scoring='accuracy',\n",
    "                       cv=kfolds.split(X_train_cat_pca, y_train_cat))\n",
    "\n",
    "clf.fit(X_train_cat_pca, y_train_cat)\n",
    "\n",
    "y_true, y_pred = y_test_cat, clf.predict(X_test_cat_pca)\n",
    "print()\n",
    "print('Accuracy', np.round(accuracy_score(y_true,y_pred),3))\n",
    "print('Recall', np.round(recall_score(y_true,y_pred, average='weighted'),3))\n",
    "print('Precision', np.round(precision_score(y_true,y_pred, average='weighted'),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  - **4.3.b)  Random Forest Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = {'n_estimators':[20,40,50,57],\n",
    "                    'max_depth':[5,10,15,20],\n",
    "                    'random_state':[42],\n",
    "                    'criterion':['mse','mae']}\n",
    "\n",
    "rg = GridSearchCV(RandomForestRegressor(), \n",
    "                   tuned_parameters,\n",
    "                   cv=10,\n",
    "                   scoring='r2')\n",
    "\n",
    "rg.fit(X_train_rev_pca, y_train_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R squared:  0.604\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = y_test_rev, regr.predict(X_test_rev_pca)\n",
    "\n",
    "print('R squared: ', np.round(r2_score(y_true, y_pred),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw some final conclusions about this project work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - For *task 1*, we discovered and visualized the data to gain some initial insights. We then prepared the data for our ML algorithms by tackling missing values (dropping some features and replacing some missing missing values with the mean) and by applying feature selection (creating dummy variables). Since we were asked to perform both classification and regression tasks, we created a new target feature revenue_category and used the stratified sampling method to ensure that the right number of instances are sampled from each of the 3 categories (<12K, 12K-20K, >20K) such that the test set was representative of the overall population. \n",
    "> - Lastly, we split the dataset into multiple subsets (training set 80% and test set 20%) and due to the high number of features present we applied PCA for both classification and regression data, being left with (X_train_cat_pca, y_train_cat, X_test_cat_pca, y_test_cat) and (X_train_rev_pca, y_train_rev, X_test_rev_pca, y_test_rev) respectively.\n",
    "\n",
    "> - For *task 2*, we selected the following models: **logistic regression, svc** and **decision tree classifier**. We also selected accuracy, recall and precision as our performance measures for the classification models. We also got the confusion matrix for each one of them alongside the best parameter combination (that yielded the highest accuracy score). The model that performed best was the decision tree classifier, with an accuracy score of 0.717, behing the logistic regression (0.705) and the svc (0.675).\n",
    "\n",
    "> - For *task 3*, we selected the following models: **lasso regression, svr** and **decision tree regresor**. We also selected R squared as our performance measure for the regression models. The model that performed best was the lasso regressor, with a R-squared score of 0.791, behing the decision tree (0.555) and the svr (0.478).\n",
    "\n",
    "> - For *task 4*, in terms of the classification problem, we explored the hard voting method for our classification voting ensemble which involves summing the predictions for each class label and predicting the class label with the most votes. The voting classifier performed worse when compared to the individual models, only achieving an accuracy equal to 0.58, which is slightly better than random guessing. For further work, to potentially improve the performance of the voting classifier, we think it could be worth to weight the contribution of each of the single classification models to the final ensble, since we know that the logistic regression and the decision tree classifier are performing relatively well (with an accuracy 0.705 and 0.717 respectively). \n",
    "> - Next, we trained an xgboost classifier model, which is an ensemble model that can combine several predictors that are trained sequentially, each trying to correct the residual errors made by the previous predictor. We used all the default parameters, including the tree_method one in which xgboost will choose the most conservative option available. We only chose merror (mlticlass classification error rate) as the evaluation metric to fit xgboost. The **accuracy score** was **0.726**, as such, **xgboost classifier outperformed all the individual classifiers as well as the voting classifier**. For time saving purposes we did not tweak any further parameters but we thought it would be important to point out that we could further analyse the model's performance with the early_stopping_round, learning_rate, n_estimators and max_depth parameters changed. \n",
    "> - Lastly, we also used the random forest classifier model trained using the bagging method. While the dt model searches for the best feature when splitting a node, the rf model searches for the best feature among a random subset of features. The model's accuracy score was 0.709.\n",
    "\n",
    "> - For *task 4*, in terms of the regression problem, we trained our voting regressor with the models from task 3 and the predictions we got were the average of the simple regressors. With an R squared of 0.57, the voting regressor achieved a better performance when compared to the svr (R2=0.478) and the dt (R2=0.555) models. The xgboost regressor's and random forest's R2 score was 0.682 and 0.604, respectively. Overall, the model that ended up **performed the best was the lasso regressor** **(R2=0.791)**, a regularized version of the linear regression model, which performs automatically feature selection. This could potentially point out the fact that even after performing PCA, our data still had least important features which were elimated by the L1 penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
